{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b2ec98",
   "metadata": {},
   "source": [
    "# TOPIC INTRODUCTION AND SOCIAL SCIENCE CONTEXT\n",
    "\n",
    "## The Case Study with #WhatsHappeninginThailand Hashtag\n",
    "\n",
    "Name: Tunyanan Pimonbutpong\n",
    "\n",
    "Link for download dataset: https://drive.google.com/file/d/1ngd1QQk-SPphEHsB7IZ2srauy4QCuOzg/view?usp=sharing\n",
    "\n",
    "or Github link: https://github.com/Tunyananp/nlpprojects_SIMM71.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c504ee5",
   "metadata": {},
   "source": [
    "Thailand has a long history of political instability, with strong military culture and monarchical overreach periodically disrupting democracy over the past century. Social media and digital activism have gradually shaped and amplified the nation's political and social discourse over the past decade. Since 2018, Thailand's adoption of Twitter has led to a significant increase in the platform's popularity (Kemp, 2020). Thais on Twitter have, to date, use the platform to discuss controversial and sensitive topics. The #WhatsHappeninginThailand is one of the popular hashtags on Twitter that Thais mainly use English for drawing international attention to know more about the situation in Thailand and publicise protest events (Sombatpoonsiri, 2020).\n",
    "\n",
    "This notebook is based on the connective action framework in the article by Bennett, W. L., & Segerberg, A. (2012). The Logic of Connective Action: Digital Media and The Personalization of Contentious Politics. Information, communication & society, 15(5), 739- 768.\n",
    "\n",
    "Here, I use the connective action framework as a foundation, which argue that internet users engage in digital activism through self-motivation and individualized activities, without the necessity for traditional organizations to function as intermediaries (Bennett and Segerberg, 2012). To achieve their collective goal, users in the connective action framework can generate their own content under the common hashtag. I proposes that Thai users use Twitter to develop their own narratives via shared hashtags in order to discuss and mobilize support for common aims. From this assumption, I would like to find the major topics that are being discussed in #WhatsHappeninginThailand hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabe107",
   "metadata": {},
   "source": [
    "# IMPORT TEXT FILES (101 TWEETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238e8fd",
   "metadata": {},
   "source": [
    "The first step, I will import a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datathailand = pd.read_csv(\"whathappenthai.csv\")\n",
    "\n",
    "df = pd.DataFrame(datathailand)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ce352",
   "metadata": {},
   "source": [
    "# DATA CLEANING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f07bcb",
   "metadata": {},
   "source": [
    "Raw tweets without preprocessing are unstructured and contain redundant and often problematic information. This dataset contains hashtag (#whatishappeninginthailand) and emojis, therefore, I decided to clean them out as they may not be necessary for topic modeling approach because those terms are not provide meaningful context for discovering inherent topics from the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba64d88",
   "metadata": {},
   "source": [
    "Firstly, I will clean the data from emojis\n",
    "\n",
    "Applied the code from jfs (2015).Removing emojis from a string in Python [Source code]. https://stackoverflow.com/a/49146722/330558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e284497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji==1.7\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "call_emoji_free = lambda x: remove_emoji(x)\n",
    "df['emoji_free_tweets'] = df['post_text'].apply(call_emoji_free)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec19bb",
   "metadata": {},
   "source": [
    "Next, the hashtag (#WhatsHappeninginThailand) will be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6803eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "call_hashtag_free = lambda x: remove_hashtags(x)\n",
    "df['final_tweets'] = df['emoji_free_tweets'].apply(call_hashtag_free)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815fd05",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING\n",
    "\n",
    "After cleaning data, the next step is data pre-processing stage, which is a stage that using for converting sentences into words, converting words to their root form and removing words that are too common or too irrelevant to the purpose of topic modelling. This process includes tokenization and word normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24fe79",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "In order to build up a vocabulary, the first thing is to break our tweets into chunks. Thus, in the first step, I will tokenize tweets by applying the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['final_tweets']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e0297",
   "metadata": {},
   "source": [
    "We will see that the tweets in the tokenized column are now separated into chunks. However, there are still stopwords and punctuation, so I will then delete them because they are common words that not necessary and do not carry a lot of information (Kedia, 2020, pp.66-67).\n",
    "\n",
    "Furthermore, as the words in the dataset include both uppercase and lowercase, I will standardize them by making all the word lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447590ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and lowercase words\n",
    "\n",
    "def remove_stopwords(tokenized_column):\n",
    "    \"\"\"Return a list of tokens with English stopwords removed. \n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column of tokenized data from tokenize()\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list with stopwords removed.\n",
    "\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [word.lower() for word in tokenized_column if word.lower() not in stop_words]\n",
    "\n",
    "df['stopwords_removed'] = df.apply(lambda x: remove_stopwords(x['Tokenized']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "string_1 = string.punctuation\n",
    "\n",
    "def remove_punctuations(punc_col):\n",
    "    string_1 = string.punctuation\n",
    "    return [word for word in punc_col if not word in string_1]\n",
    "\n",
    "df['punc_removed'] = df.apply(lambda x: remove_punctuations(x['stopwords_removed']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8defc",
   "metadata": {},
   "source": [
    "In addition to this, the dataset contains a great number of the country name (Thailand) and nationality (Thai). moreover, there are also number. I decide to remove these because they do not give us much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494172c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete words thailand and thai\n",
    "\n",
    "Thailand = '''thailand, thai, thais'''\n",
    "\n",
    "def remove_thai(thai_col):\n",
    "    Thailand = '''thailand, thai, thais'''\n",
    "    return [word for word in thai_col if not word in Thailand]\n",
    "\n",
    "df['thai_removed'] = df.apply(lambda x: remove_thai(x['punc_removed']), axis=1)\n",
    "\n",
    "# Delete number\n",
    "\n",
    "def remove_number(num):\n",
    "    return [x for x in num if not x.isdigit()]\n",
    "\n",
    "df['number_removed'] = df.apply(lambda x: remove_number(x['thai_removed']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f98bd",
   "metadata": {},
   "source": [
    "\n",
    "## Word Normalization\n",
    "\n",
    "After I tokenized tweets, now I will lemmatize tweets into their root forms using Spacy Lemmatizer. This helps into reducing the amount of different information that the computer has to deal, and therefore improves efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb178c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load('en')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['number_removed']]\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e208d",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d6df5",
   "metadata": {},
   "source": [
    "Next, I will conduct the topic modeling with Gensim. This topic model based on the Latent Dirichlet Allocation (LDA) algorithm, which is unsupervised machine learning. I will use Genism to create the bag-of-words that form the corpus. Firstly, we need to build the dictionary to have the corpus, as the corpus is made from documents converted to bag-of-words, and a dictionary is required for building bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dictionary \n",
    "\n",
    "dictionary = Dictionary(df['lemmas'])\n",
    "\n",
    "#Use the dictionary to generate the corpus (set of bag-of-words model)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['lemmas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366dd6e1",
   "metadata": {},
   "source": [
    "After we have built the corpus, topic coherence is one of the main techniques used to estimate the number of topics. Thus, in the next step, I will decide the number of topics based on the calculation of the coherence score using C_v.and plot the coherence model using seaborn.\n",
    "\n",
    "Codes from Ghanoum.T (2021). Topic Modelling in Python with spaCy and Gensim.[Source code]. https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis \n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()# Visualise inside a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "score = []\n",
    "for i in range(1,10,1):\n",
    "   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
    "   cm = CoherenceModel(model=lda_model, texts = df['lemmas'], corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "   topics.append(i)\n",
    "   score.append(cm.get_coherence())\n",
    "_=plt.plot(topics, score)\n",
    "_=plt.xlabel('Number of Topics')\n",
    "_=plt.ylabel('Coherence Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9cd01",
   "metadata": {},
   "source": [
    "When looking at the coherence using C_v algorithm, I choose to go with 2 topics because it has a high coherence score with around 0.45.  The last step, I will train my topic model with the number of topics equal two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Model\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=100, num_topics= 2, workers = 4, passes=10, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d5901",
   "metadata": {},
   "source": [
    "# VISUALIZATION\n",
    " \n",
    "After finalize the number of topics, I will visualize the topic model using pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e62535",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edbc6f",
   "metadata": {},
   "source": [
    "The visualization shows two bubbles, in which each bubble represents a topic. When looking at the red bars that give the estimated number of times a given term was generated by a given topic, the first topic is related to people's demands (keywords such as want and need). The keywords indicate that Thais would like the problems of human rights, economy, democracy, freedom of expression, law, mistreatment and violence against protesters caused by police and the military government to be resolved.\n",
    "\n",
    "Next, the keywords under the second topic is in regard to the state violence and police brutality. They describe what force the government and police used (e.g., shoot, bullet, arrest, and abuse). They also highlighted the victims being protesters and in particular students. This might be because since 2018, after twitter has gradually shaped and amplified the nation's political and social discourse, protest events in Thailand often lead by student movement from various campuses (Sombatpoonsiri, 2020).\n",
    "\n",
    "In conclusion, this project has shown the major themes hashtag activists addressed on Twitter. In the connective action framework, users can generate their own content under the common hashtag to achieve their collective goal. In other words, there are multiple issues within the hashtag that can be connected to common concerns. Topic modeling indicated that activists within #WhatsHappeninginThailand hashtag focused on multiple issues  which were connected through a common concern, i.e. state violence and police brutality. Furthermore, they utilized Twitter as a platform to demand for reforming the country system that full of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b1bb1",
   "metadata": {},
   "source": [
    "# REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceae0b6",
   "metadata": {},
   "source": [
    "Bennett, W. L., & Segerberg, A. (2012). The Logic of Connective Action: Digital Media and The Personalization of Contentious Politics. Information, communication & society, 15(5), 739- 768.\n",
    "\n",
    "Kedia, A., & Rasu, M. (2020). Hands-on Python natural language processing: explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing Ltd.\n",
    "\n",
    "Kemp, S. (2020, February 18). Digital 2020: Thailand. https://datareportal.com/reports/digital-2020-thailand.\n",
    "\n",
    "Sombatpoonsiri, J. (2020, September, 20). Unpacking Thailandâ€™s Protests: Current Contour and Future Trajectories. https://www.ispionline.it/en/pubblicazione/unpacking-thailands-protests-current-contour-and-future-trajectories-27300\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
